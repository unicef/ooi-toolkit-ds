<!doctype html><html lang=en-us><head><meta charset=utf-8><title>UNICEF Data Science & A.I. Toolkit</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.96.0"><meta name=description content="UNICEF Data Science & A.I. Toolkit - UNICEF Data Science & A.I. Toolkit "><link rel=stylesheet href=https://unicef.github.io/ooi-toolkit-ds/plugins/bootstrap/bootstrap.min.css><link rel=stylesheet href=https://unicef.github.io/ooi-toolkit-ds/plugins/themify-icons/themify-icons.css><link rel=stylesheet href=https://unicef.github.io/ooi-toolkit-ds/plugins/drone-dpgtoolkit-icons/icons.css><link rel=icon href=https://unicef.github.io/ooi-toolkit-ds/images/favicon.png type=image/x-icon><link href="https://fonts.googleapis.com/css?family=Open%2bSans:300,400,700&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.9.3/html2pdf.bundle.js></script>
<script src=https://kit.fontawesome.com/9196e0632a.js crossorigin=anonymous></script><style>:root{--primary-color:#1CABE2;--primary-color-light:#1b66b1;--body-color:#f2f4f6;--text-color:#636363;--text-color-dark:#374ea2;--text-title-color:#ffffff;--white-color:#ffffff;--light-color:#f8f9fa;--font-family:Open+Sans}</style><link href=https://unicef.github.io/ooi-toolkit-ds/css/style.min.css rel=stylesheet media=screen><script src=https://unicef.github.io/ooi-toolkit-ds/plugins/jquery/jquery-1.12.4.js></script>
<script src=https://unicef.github.io/ooi-toolkit-ds/plugins/jquery/jquery-ui.js></script>
<script src=https://unicef.github.io/ooi-toolkit-ds/plugins/bootstrap/bootstrap.min.js></script>
<script src=https://unpkg.com/@popperjs/core@2></script>
<script src=https://unicef.github.io/ooi-toolkit-ds/plugins/match-height/jquery.matchHeight-min.js></script><meta name=twitter:title content="UNICEF Data Science & A.I. Toolkit"><meta name=twitter:description content="A toolkit for data science and AI modeling best practices, created for the UNICEF Venture Fund in the Office of Innovation."><meta property="og:title" content="UNICEF Data Science & A.I. Toolkit"><meta property="og:description" content="A toolkit for data science and AI modeling best practices, created for the UNICEF Venture Fund in the Office of Innovation."><meta property="og:type" content="website"><meta property="og:url" content="https://unicef.github.io/ooi-toolkit-ds/"><meta name=twitter:card content="summary"><meta name=twitter:title content="UNICEF Data Science & A.I. Toolkit"><meta name=twitter:description content="A toolkit for data science and AI modeling best practices, created for the UNICEF Venture Fund in the Office of Innovation."></head><body><header class="banner overlay bg-cover" data-background=https://unicef.github.io/ooi-toolkit-ds/images/banner.jpg><nav class="navbar navbar-expand-md navbar-dark" style=background-color:transparent><div class="container px-2 px-md-0 navigation-bar"><div id=site-brand><a class="navbar-brand px-2" href=https://www.unicef.org/><div class=text-center><img class="img-fluid d-inline" src=https://unicef.github.io/ooi-toolkit-ds/images/unicef-logo.png alt="UNICEF Data Science & A.I. Toolkit"> <a class="text-white d-block" href=/ooi-toolkit-ds>UNICEF Data Science & A.I. Toolkit</a></div></a></div><button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation aria-controls=navigation aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse text-center" id=navigation><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link text-white" href=https://unicef.github.io/ooi-toolkit-ds/faq>FAQ</a></li><li class=nav-item><a class="nav-link text-white" href=https://unicef.github.io/ooi-toolkit-ds/pages>pages</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle text-white" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Toolkits</a><div class=dropdown-menu><a class=dropdown-item href=https://unicef.github.io/drone-4sdgtoolkit/>Drones</a>
<a class=dropdown-item href=https://unicef.github.io/inventory/>Open Source</a>
<a class=dropdown-item href=https://unicef.github.io/ooi-toolkit-software/>Software Development</a></div></li></ul></div></div><p class="text-white unicef-glob mx-3">Visit <a href=https://www.unicef.org/ class=text-white>UNICEF Global <i class="fas fa-angle-double-right"></i></a></p></nav><div class="container section"><div class=row><div class="col-lg-8 text-center mx-auto"><h1 class="text-white mb-3">UNICEF Data Science & A.I. Toolkit</h1><p class="text-white mb-4">A toolkit for data science and AI modeling best practices, created for the UNICEF Venture Fund in the Office of Innovation.</p><div class=position-relative><input id=search class=form-control placeholder="Have a question? Search the site here.">
<i class="ti-search search-icon"></i>
<script>$(function(){var e=[{value:"AI Ethics \u0026 Transparency Roadmap - Template",label:"<p>How do you document your machine learning development process? This guide shares transparency best practices.\nThis is your at a glance version of the document. Details and resources are below.\n Milestone 1: Understanding the Data Flow  Data Ecosystem Map Information Sharing Protocol   Milestone 2: Understanding the Algorithm  Dataset Structure Common Traps Mitigations   Milestone 3: Sharing the Model  Model Card Created    Understanding Data Flow  Primary Goal: Create documentation that ensures you know what data you\u0026rsquo;re using and how you plan on sharing it.  This first milestone is about initially understanding the underlying data powering your model. This is generally important for understanding the plumbing and getting an idea about privacy and implications that may arise from sharing data.\nOutcomes  Data Ecosystem Map: This is to document where their data is coming from, what is using it, stakeholders, etc. The benefit here is not just helping organize how data collection will work in production but also projecting partnerships which may need to be formed in the future. Information Sharing Protocol: This document is all about conducting an initial assessment of the sensitivity of information you are collecting, who you want to share it with, and how. It is fairly well-thought but is worth a review and potentially some edits. If you are in the EU, it might just be better to do a Data Protection Impact Assessment.  Understanding the Algorithm  Primary Goal: Create documentation to ensure your whole team understand exactly how your model will be working.  A large part of this will be determining the method of managing your datasets as they evolve over the development of the program.\nOutcomes  Determine Dataset Structure: As you create new data sets, update them, and remove old datasets, you should follow an easy to follow protocol for keeping track of your various data sets as they are updated and deprecated. Common Traps When Building Models: Review the following traps and document potential risks of your application falling into any of these traps.  Documenting the Algorithm  Primary Goal: Produce documentation that should be shared with the model.  Outcomes  Document Machine Learning Model Card: Before you release or begin production use of your machine learning model, you can use the linked template to comprehensively document the effects, background, and purpose of your model. This will serve as a core document showing how the model makes the decision it does.  </p>",url:"https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/milestone-roadmap/"},{value:"Alerts",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/alerts/"},{value:"Categories",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/categories/"},{value:"Common traps to avoid when building AI systems",label:"<p>Assessing common mistakes that lead to unintended side effects. Information summarized from Fairness and Abstraction in Sociotechnical Systems, a paper published at the 2019 ACM Conference on Fairness, Accountability, and Transparency.\nFraming Trap Failure to model the entire system over which a social criterion, such as fairness, will be enforced.\nFor this trap, we will want to look at our outcome variables. Are these variables a proxy of the actual outcome you wish to achieve? What evidence of existing negative bias currently exists with regards to these variables?\nPortability Trap Failure to understand how repurposing algorithmic solutions designed for one social context may be misleading, inaccurate, or otherwise do harm when applied to a different context.\nHere, you will want to be fully understanding of the context in which this model is being built for and the context in which you will be using it. There should be clear documentation distributed across the entire team focusing on this. What stakeholders do you expect to have an impact on where this technology will be used. Are they informed?\nFormalism Trap Failure to account for the full meaning of social concepts such as fairness, which can be procedural, contextual, and contestable, and cannot be resolved through mathematical formalisms.\nHow does your chosen outcome continue to implement and solve existing procedural \u0026ldquo;catches\u0026rdquo; in order to ensure the decision making process is fair? What method of recourse are available for those who are unfairly judged?\nRipple Effect Trap Failure to understand how the insertion of technology into an existing social system changes the behaviors and embedded values of the pre-existing system.\nHow do you think the introduction of this recommendation system will affect your users? What changes in behavior do you intend to change? Can you think of any possible changes in behavior that you don\u0026rsquo;t intend as a result of your software?\nSolutionism Trap Failure to recognize the possibility that the best solution to a problem may not involve technology.\nWill this technology elevate social values which can be quantified? Will it devalue those which cannot? What values might take a back seat if this technology is implemented?\nAttributions Thanks to the publishers of the original paper:\n Andrew D. Selbst (UCLA School of Law) Danah Boyd (Data \u0026amp; Society Research Institute; Microsoft Research) Sorelle Friedler (Haverford College) Suresh Venkatasubramanian (University of Utah) Janet Vertesi (Princeton University)  </p>",url:"https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/traps/"},{value:"Data Collection and Processing",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/categories/data-collection-and-processing/"},{value:"Data Collection and Processing",label:"<p>This toolkit will draw on considerations in data collection and processing for your data science project.\nData science naturally relies on collecting data from some source. Typically, data is generated by human activity, such as logs of activity on a web platform (for e.g., clicks, score records, etc.), recorded events (for e.g., visits to medical facilities, attendance at school, etc.), text scripts, visual images or audio recordings. Most data generated in this way is typically observational whereby the data collector has no control over the data generating process that occurs in the real world. Observational data is collected based on what is seen or heard by people or a computer passively observing some process.\nData can also be experimental, whereby data is collected in a controlled environment following a scientific method. Experimental data is not passively collected, but rather it is collected methodically to answer a specific question typically in a controlled setting. In experimental settings, a group of people or things are randomly assigned to treatment and control groups. For example, in drug trials, a treatment group is given some dosage of a drug, while a control group would be assigned a placebo. Experiments lend themselves best to cause-and-effect studies because assigned to treatment and control are randomized and therefore, latent confounding factors that might differentiate the groups can be controlled or explicitly identified. In contrast, cause-and-effect claims cannot be made readily with observational data unless all explicit and latent factors can be controlled for and there is some source of external source of variation that can be attributed directly to the effect (a challenging analytical task).1 Controlled experiments of this type are often used in A/B testing and user design testing.\nAnother consideration is whether the data is structured or unstructured. Structured data has a sense of order and is typically in a row-column structured framework such as spreadsheets or database tables. Unstructured data can be images, text scripts or audio files. Unstructured data can require additional processing to convert their attributes into structured data format for analysis methods.\nAttribution: 1Gerber, Alan S., and Donald P. Green. Field Experiments: Design, Analysis, and Interpretation. W.W. Norton, 2012.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/"},{value:"Data Governance",label:"<p>Data governance is a critical component of a sustainable and scalable AI project. Data governance is defined as a collection of processes, roles, policies, standards and metrics that ensure the effective and efficient use of information. Good data governance establishes processes and responsibilities ensuring data quality and security. Essentially, data governance defines who can act on the data, upon what data, in which situations and with what methods.\nAttribution:\nEvren Eryurek, et al. Data Governance: The Definitive Guide: People, Processes, and Tools to Operationalize Data Trustworthiness. O’Reilly Media, 2021.\nBuilding a data governance framework - Talend Data Integration. Talend. (n.d.). Retrieved from https://www.talend.com/resources/building-data-governance-framework/\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/data-governance.en.adoc/"},{value:"Data Pre-processing",label:"<p>Data pre-processing has to do with the process of managing, analyzing, filtering, transforming, encoding and preparing data to be usefully processed by the machine. This is typically one of the most time-consuming aspects of the data science process. Any decisions made by the data scientist in the data pre-processing stage can have important impacts in subsequent model development and, even, deployment stages. Therefore, it is important to document analytical decisions made in this stage, whether in the code or through a documenting tool, to trace the lineage of the data from the original data source, when the data is in its raw form, to the model training and development stage.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/data-preprocessing.en.adoc/"},{value:"Data Privacy \u0026 Ethics",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/"},{value:"Data Quality",label:"<p>Data quality is a critical concern for AI projects. Poor data quality can bias prediction results and mislead users. There are six key areas of data quality:\nAccuracy: how well does the data reflect reality?\nCompleteness: is the data comprehensive or not missing value entries in unexpected instances?\nConsistency: does information stored in one place match the information stored in another place?\nTimeliness: is data available when needed?\nValidity: is data in a specific format? Is it an unusable format? Does it follow business rules?\nUniqueness: Is the data instance the only instance in which the data appears in the sample?\nAttribution:\nSarfin, R. L., \u0026amp; Editor, P. (2021, May 12). Data Quality Dimensions: How do you measure up? (\u002b free scorecard). Precisely. Retrieved from https://www.precisely.com/blog/data-quality/data-quality-dimensions-measure\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/data-quality.en.adoc/"},{value:"Definitions",label:"<p>It is important to start with some definitions of the terminology used in this toolkit.\nData science is broadly defined as a craft or set of activities that involves working with large amounts of data, grappling with computational problems introduced by structure, size, messiness such as missing data, and the complexity of data, while attempting to solve a real-world problem. Product-oriented data science deals with everything from the data engineering and infrastructure for data collection and logging, to privacy considerations, to decisions around what data is user-facing, how and which data will be used to inform decisions, and how the data will be built back into the product.1\nArtificial intelligence (AI) is broadly defined as an effort to automate intellectual tasks normally performed by human beings. AI includes machine learning and deep learning as well as approaches that do not involve any learning. Symbolic AI, for example, is premised on the ability of programmers to hard code a sufficiently large number of rules for manipulating knowledge (this is also known as expert systems). Symbolic AI can serve well for logical, well-defined problems.2\nMachine learning (ML) is better suited for more complex and intractable problems, such as image classification, speech recognition, language translation and probabilistic anomaly detection. Machine learning is a subset of AI where data can be used to train computers to detect patterns and mimic human decision-making. A machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. Machine learning involves using algorithms to detect patterns in data and then use those learned patterns to predict an outcome on new data. Generally, this involves an iterative process and allows the machine learning system to determine predictions without being explicitly programmed.\nDeep learning is a subset of ML that can process a wider range of data resources including text, images and audio clips and typically requires less data preprocessing by humans. The “deep” part refers to learning successive layers of increasingly meaningful representations all learned from training data. The depth refers to the number of layers that contribute to a model of the data. In contrast, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; hence, they’re sometimes called shallow learning. In common-speak, the application of neural networks models refers to deep learning.\nNatural Language Processing (NLP) heavily leverages Machine Learning as well as Deep Learning to allow computers to interact with humans in their own natural language whether written or oral. NLP applications attempt to understand natural human communication, either written or spoken, and communicate in return with us using similar, natural language.\nRobotic Process Automation (RPA) is a type of automation that mimics the activity of a human when they carry out a structured, repetitive task or process, little judgement. It is generally rule based, can’t learn outside parameters, so not considered AI; however, RPA can be built with learning modules on complex tasks.\nFigure 1 diagram below depicts the AI ecosystem and the relationship between AI, ML, Deep Learning, NLP and RPA.\nFootnotes\n1 Schutt, Rachel, and Cathy O\u0026rsquo;Neil. Doing Data Science: Straight Talk from the Frontline. O\u0026rsquo;Reilly, 2013.\n2 Chollet François. Deep Learning with Python. Manning Publications, 2021.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/01-definitions/"},{value:"DownloadBtn",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/downloadbtn/"},{value:"Feature Engineering",label:"<p>Feature engineering is related to data pre-processing but is more closely related to the modeling process. Feature engineering refers to when model inputs or features are generated that are then incorporated into a model for training. Typically, this means adjusting and reworking the predictors to enable models to better uncover predictor-response relationships. The engineering involves the steps taken to improve model performance by generating the appropriate features. Since we may not know in advance what adjustments or re-adjustments of the features are required to improve model performance, the re-working of predictors can be experimental in nature requiring experience and tools to find predictor representations.\nFeature engineering can involve scaling (to scale numerical features so that they are in the same numerical scale) through standardization or normalization. Normalization involves mean scaling where the mean value of a particular feature is subtracted from each feature value. Standardization involves variance scaling where each feature has a mean value of zero and standard deviation of one.\nIf a feature can take on many values, it can be normalized with buckets. For example, if a feature is real-valued, ordinal, or integer valued, the values can be arranged into several different buckets. In some cases, features can be encoded into binary values (0 or 1, “Yes” or “No”, null or not null). The latter is typically the case with text features.\nAttribution:\nJohnson, K. and Max Kuhn. (2019, June 21). Feature engineering and selection: A practical approach for predictive models. Feature Engineering and Selection: A Practical Approach for Predictive Models. Retrieved from http://www.feat.engineering/\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/feature-engineering.en.adoc-copy/"},{value:"Machine Learning Lifecycle",label:"<p>This toolkit will emphasize the key aspects of the machine learning project lifecycle. This starts with scoping (defining the project), data collection and organization, model development and model deployment.\nFigure 2. The Machine Learning Lifecycle\nMachine learning projects typically start with scoping or defining the project. This includes deciding on the objectives of the project and what makes it a good candidate for a machine learning application.\nThen, it is defining the data and establishing a baseline. For example, if we can accurately predict an outcome 50% of the time just by flipping a coin, a model should, at least, do better than that. Features should be engineered attuned to the underlying characteristics of the data (for instance, different treatment of categorical versus numerical data types) and business-specific features related to the defined problem. Often, data exploration and data mining inform the data scientist about the features to craft. Moreover, we should ensure the data is labeled consistently in the case of a supervised learning problem.\nThe modeling process involves selecting an algorithm or model framework (what algorithmic framework makes sense for the problem and data at hand) and training the model to learn the representations in the input data that get us closer to the output data. Error analysis is performed to evaluate and improve the model. Typically, the model that best minimizes the error is selected for deployment. Often, modeling informs us about issues in the data organization and labeling, leading us to go back and fix issues in the data.\nDeployment is typically the final step in the process. Once the model is selected, and it is put into production, it should be continually monitored through error analysis and the entire system should be maintained to detect any changes in the underlying data distribution serving as inputs to the model. Over time, it is common for machine learning models to require maintenance and retraining (reverting back to model development and then re-deployed).\nAttribution\nNg, Andrew \u0026amp; Cristian Bartolomé Arámburu (n.d.). Introduction to Machine Learning in Production [MOOC]. Coursera DeepLearning.AI. https://www.coursera.org/learn/introduction-to-machine-learning-in-production\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/02-machine-learning-lifecycle/"},{value:"Machine Learning Model Card",label:"<p>The following is an editable version of the model card proposed in arxiv.org/abs/1810.03993.\nModel details Basic information about the model.\n Person or organization developing model Date of last update Model version Model type (information about training algorithms, parameters, fairness constraints or other applied approaches, and features) Paper or other resource for more information Citation details License Maintainer contact details  Intended use Use cases that were envisioned during development.\n Primary intended uses Primary intended users Out-of-scope use cases  Factors Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others.\n Relevant factors Evaluation factors  Metrics Metrics should be chosen to reflect potential real-world impacts of the model.\n Model performance measures Decision thresholds Variation approaches  Evaluation data Details on the dataset(s) used for the quantitative analyses in the card.\n Datasets Motivation Preprocessing  Training data May not be possible to provide in practice. When possible, this section should mirror Evaluation Data. If such detail is not possible, minimal allowable information should be provided here, such as details of the distribution over various factors in the training datasets.\nQuantitative analyses Quantitative analyses should be broken down by the chosen factors.\n Unitary results Intersectional results  Ethical considerations In this section, you should explore and document the following:\n Sensitive Data: Does the model use any sensitive data (e.g. protected classes)? Human life: Is the model intended to inform decisions about matters central to human life or flourishing – e.g., health or safety? Or could it be used in such a way? Mitigations: What risk mitigation strategies were used during model development? Risks and harms: What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown. Use cases: Are there any known model use cases that are especially fraught?  Caveats and recommendations This section should list additional concerns that were not covered in the previous sections.\nTo be written.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/model-card/"},{value:"Missing Data",label:"<p>Handling missing values can be a component of data pre-processing as well as feature engineering. Missing values can occur for several reasons including structural deficiencies in the data (namely, deliberate omission in the data collection or data generating process), random occurrences, or specific reasons.\nA structural deficiency would be the case of a missing component of a predictor omitted from the data. Typically, these cases could be resolved once the necessary component is identified. For example, if a hypothetical dataset has some predictor variable where values are either “A” or “B” or missing, where most values are missing, it may be tempting to discard the predictor variable due to the high rate of missingness. However, discarding this predictor variable may inadvertently be throwing away valuable information since missing can be interpreted to mean not being “A” or “B”. A better recording of the predictor variable may be to replace the missing values with “Not A or B”.\nAnother reason for missing values may be due to random occurrences. This can be occurrences missing completely at random (“MCAR”) where the likelihood of a missing value is equal for all data points, both observed and unobserved. Missing values can be interpreted here as independent of the data generating process. This can also be an occurrence missing at random (“MAR”) where the likelihood of a missing value is not equal for all data points. In this case, the probability of a missing value depends on the observed data, but not unobserved data.\nA third reason for missingness can be due to specific reasons or missing not at random (“MNAR”). This often occurs in cross-sectional time series data where the same unit of observation (for example, patient in medical trial) drops out of a study. Measurements for this unit of observation will stop after dropping out of the study. The MNAR cases are difficult to handle in practice and data practitioners should seek to understand the nature of the missingness before applying a technique to correct for missingness since a misdiagnosis of missingness could lead to bias in model inference.\nAttribution:\nJohnson, K. and Max Kuhn. (2019, June 21). Feature engineering and selection: A practical approach for predictive models. Feature Engineering and Selection: A Practical Approach for Predictive Models. Retrieved from http://www.feat.engineering/\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/missing-data.en.adoc/"},{value:"Policy guidance on AI for children (via unicef.org)",label:"<p>As part of our AI for children project, UNICEF has developed this policy guidance to promote children\u0026rsquo;s rights in government and private sector AI policies and practices, and to raise awareness of how AI systems can uphold or undermine these rights. The policy guidance explores AI systems, and considers the ways in which they impact children.\nRead in full: unicef.org/globalinsight/reports/policy-guidance-ai-children Drawing on the Convention on the Rights of the Child, the guidance offers nine requirements for child-centered AI:\n Support children’s development and well-being Ensure inclusion of and for children Prioritize fairness and non-discrimination for children Protect children’s data and privacy Ensure safety for children Provide transparency, explainability, and accountability for children Empower governments and businesses with knowledge of AI and children’s rights Prepare children for present and future developments in AI Create an enabling environment  To support implementation of the guidance, a list of online resources and a set of practical implementation tools are provided, including:\n Roadmap for policymakers AI for children development canvas AI guide for parents AI guide for teens  To see how the guidance has been applied in practice, read about the eight case studies.\nRead more on unicef.org.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/policy-guidance-children/"},{value:"privacy-ethics",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/categories/privacy-ethics/"},{value:"Scoping",label:"<p>Scoping has to do with deciding on the objectives of the project and what makes it a good candidate for a machine learning or artificial intelligence application. Scoping can seem to be the most trivial aspect of a project, yet it involves the most fundamental part in initiating a project. There are a few questions a project team should ask before initiating a data problem:\n  Why is the problem important?\n  Who does the problem affect?\n  What if we don’t have the right data to solve the problem?\n  When is the project over?\n  What if we don’t like the results?\n  Attribution: Gutman, A. J., \u0026amp; Goldmeier, J. (2021). Becoming a data head: How to think, speak, and understand data science, statistics, and Machine Learning. John Wiley \u0026amp; Sons.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/"},{value:"true",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/downloadbtn/true/"},{value:"UNICEF Data Science \u0026 A.I. Toolkit",label:"<p></p>",url:"https://unicef.github.io/ooi-toolkit-ds/"},{value:"What if we don’t have the right data to solve the problem?",label:"<p>This has to do with inherent limitations that may exist in the information collected. At some point, no technology or analysis will help you further. Considering whether the right data exists is critical at the early project stage. You may want to create contingencies to pivot towards collecting better data to answer the question. Or, if the data simply does not exist or is impractical to collect, revert to the original question of interest and redefine the project scope. This is about aligning expectations and gets to why the project was initiated in the first place.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/what-if-we-dont-have-the-right-data.adoc/"},{value:"What if we don’t like the results?",label:"<p>This is when the results of the project are not those anticipated by the end user. This can be a function of not having the right data, poor model inputs or poor model predictions in specific scenarios. The possibility that the project might lead to poor results should be discussed among the project stakeholders. This addresses differences in how individuals might accept the results of the project and reveal inherent biases individuals might have. This avoids initiating a project where you already know there’s only one acceptable result. </p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/what-if-we-dont-like-the-results.adoc/"},{value:"When is the project over?",label:"<p>This is about aligning expectations and gets to why the project was initiated in the first place. Think about what the final deliverable is and work iteratively backward. Gather stakeholder input and identify reasons the project could end. Setting aside obvious failures, such as lack of funding or losing interest, focus should be on the needs to be delivered and how to conclude the project. Many projects will require ongoing support and maintenance, but this should be clarified up front. The answer to when the project is over shouldn’t be assumed, until asked.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/when-is-the-project-over.adoc/"},{value:"Who does the problem affect?",label:"<p>This is about identifying the end users. Identify the people who will be affected by the problem and incorporate them into the discussion of problem articulation. This can mean small group or focus group discussions if they are representative of a larger group of people. Understand how these people will be affected by the project. One approach to think about this is to have a solution trial run. Assuming you can answer the problem, ask yourself if you can use the answer and whose life will change as a result.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/who-does-the-problem-affect.adoc/"},{value:"Why is the problem important?",label:"<p>This section is about problem definition. This sets the expectations for why a project should be undertaken in the first place. Identifying the importance of the problem before starting will help optimize how company resources are used. Relatedly, you will want to answer:\n Why does this problem matter? Is this a new problem or has it been solved already?  In this effort, you will want to engage with the different collaborators, both technical and non-technical, to understand how each person sees the problem. This is important to create alignment and support for the project to solve the problem.\nTechnology should not be included in defining the problem. Too much focus on methodology (i.e., some new analytical method or technology) or on deliverables (i.e., interactive dashboard, etc) can derail from the actual business problem. The problem should be articulated in a direct, clear language that everyone can understand. Think carefully about the problem to be solved as opposed to the technology to be used.\n</p>",url:"https://unicef.github.io/ooi-toolkit-ds/03-scoping/why-is-the-problem-important.en.adoc/"}];$("#search").autocomplete({source:e}).data("ui-autocomplete")._renderItem=function(t,e){return $("<li>").append("<a href="+e.url+' + " &quot;" +  >'+e.value+"</a>"+e.label).appendTo(t)}})</script></div></div></div></div></header><script>remSpace()</script><script>function remSpace(){document.getElementById("section").style.paddingTop="3rem"}</script><section class=section><div class=container><div class="row justify-content-center"><div class="col-12 text-center"><h2 class=section-title>Find your answer by subject</h2></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-eye icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Data Collection and Processing</h3><p class=mb-0>Considerations in data collection and data processing</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/data-preprocessing.en.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Data Pre-processing</h3><p class=mb-0>What is data pre-processing about?</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/privacy-ethics/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-eye icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Data Privacy & Ethics</h3><p class=mb-0>Develop a more ethical, transparent, and safe AI system with these resources.</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/data-quality.en.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Data Quality</h3><p class=mb-0>Data quality concerns</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/01-definitions/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Definitions</h3><p class=mb-0>Definitions of the terminology used in this toolkit</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/feature-engineering.en.adoc-copy/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Feature Engineering</h3><p class=mb-0>What is feature engineering and considerations in performing feature engineering</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/02-machine-learning-lifecycle/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Machine Learning Lifecycle</h3><p class=mb-0>Key aspects of the machine learning project lifecycle</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/04-data-collection-processing/missing-data.en.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Missing Data</h3><p class=mb-0>What is the nature of missing data? How to consider handling missing data?</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-eye icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Scoping</h3><p class=mb-0>Deciding on the objectives of the project. What makes a good candidate for a machine learning or artificial intelligence application</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/what-if-we-dont-have-the-right-data.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">What if we don’t have the right data to solve the problem?</h3><p class=mb-0>Data limitations in the data collected to solve a problem</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/what-if-we-dont-like-the-results.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">What if we don’t like the results?</h3><p class=mb-0>Reacting to poor results</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/when-is-the-project-over.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">When is the project over?</h3><p class=mb-0>Aligning project expectations</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/who-does-the-problem-affect.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Who does the problem affect?</h3><p class=mb-0>Identifying the end users</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://unicef.github.io/ooi-toolkit-ds/03-scoping/why-is-the-problem-important.en.adoc/ class="px-4 py-5 bg-white shadow text-center d-block match-height cardLink"><i class="ti-search icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Why is the problem important?</h3><p class=mb-0>Defining the problem</p></a></div></div></div></section><footer class="section bg-primary p-4 mt-5"><div class=container><div class="row align-items-center"><div class="col-md-3 text-md-left text-center"><ul><li><a class="nav-link text-white" href=/ooi-toolkit-ds>HOME</a></li><li class=nav-item><a class="nav-link text-white text-uppercase" href=https://unicef.github.io/ooi-toolkit-ds/faq>FAQ</a></li><li class=nav-item><a class="nav-link text-white text-uppercase" href=https://unicef.github.io/ooi-toolkit-ds/>Toolkits</a></li></ul></div><div class="col-md-6 text-md-center text-center"><p class="mb-md-0 mb-4 text-white">A toolkit for data science and AI modeling best practices, created for the UNICEF Venture Fund in the Office of Innovation.</p></div><div class="col-md-3 text-md-right text-center"><ul class=text-white><li class=nav-item><a class="nav-link text-white font-weight-bold" href=https://www.unicef.org/>UNICEF Global</a></li><li class=nav-item><a class="nav-link text-white font-weight-bold" href=https://www.unicefinnovationfund.org/>UNICEF Venture Fund</a></li><li class=nav-item><a class="nav-link text-white font-weight-bold" href=https://www.unicef.org/legal>Legal</a><p class="h6 text-md-right font-weight-light">Find legal information and policies related to UNICEF Global's digital communications.</p></li></ul><ul class=list-inline><li class=list-inline-item><a class="text-color d-inline-block p-2 text-white" href=https://github.com/unicef/ooi-toolkit-ds aria-label="UNICEF Data Science & Artificial Intelligence Toolkit on GitHub"><i class=ti-github></i></a></li><li class=list-inline-item><a class="text-color d-inline-block p-2 text-white" href=https://twitter.com/UNICEFinnovate aria-label="@UNICEFinnovate on Twitter"><i class=ti-twitter></i></a></li></ul></div></div></div><p class="text-md-center text-center text-white mt-3 h6 font-italic">Creative Commons BY-SA 4.0. Site theme adapted from UNICEF Inventory theme.</p></footer><script src=https://unicef.github.io/ooi-toolkit-ds/js/script.min.js></script></body></html>